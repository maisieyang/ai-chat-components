import { NextRequest, NextResponse } from "next/server";

import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { HttpResponseOutputParser } from "langchain/output_parsers";

export const runtime = "edge";

interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp?: Date;
}

const formatMessage = (message: ChatMessage) => {
  return `${message.role}: ${message.content}`;
};

const TEMPLATE = `You are a helpful AI assistant. Please provide clear and helpful responses.

Current conversation:
{chat_history}

User: {input}
AI:`;

export async function POST(req: NextRequest) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    // const lastMessage = messages[messages.length - 1];
    
    // 临时使用 Mock 模式（因为 OpenAI 配额问题）
    // const mockResponses = [
    //   `I received your message: "${lastMessage.content}". This is a mock response using LangChain framework.`,
    //   `That's interesting! You said: "${lastMessage.content}". I'm using LangChain to process this.`,
    //   `I understand you're asking about: "${lastMessage.content}". This is a LangChain-powered response.`,
    //   `Great question! Regarding "${lastMessage.content}", here's what I think using LangChain...`,
    //   `I see you mentioned "${lastMessage.content}". LangChain makes this response possible!`,
    //   `Thanks for sharing: "${lastMessage.content}". This response is generated by LangChain framework.`,
    //   `You've got me thinking about: "${lastMessage.content}". LangChain is handling this conversation.`
    // ];
    
    // 随机选择一个响应
    // const randomResponse = mockResponses[Math.floor(Math.random() * mockResponses.length)];
    
    // 模拟流式响应
    // const encoder = new TextEncoder();
    // const stream = new ReadableStream({
    //   start(controller) {
    //     // 模拟逐字输出
    //     let index = 0;
    //     const interval = setInterval(() => {
    //       if (index < randomResponse.length) {
    //         controller.enqueue(encoder.encode(randomResponse[index]));
    //         index++;
    //       } else {
    //         controller.close();
    //         clearInterval(interval);
    //       }
    //     }, 50); // 每50ms输出一个字符
    //   }
    // });
    
    // return new Response(stream, {
    //   headers: {
    //     'Content-Type': 'text/plain; charset=utf-8',
    //   },
    // });
    
    // 以下是真实的 OpenAI 调用代码（当配额问题解决后可以启用）

    const formattedPreviousMessages = messages.slice(0, -1).map(formatMessage);
    const currentMessageContent = messages[messages.length - 1].content;
    const prompt = PromptTemplate.fromTemplate(TEMPLATE);

    // 检查API密钥
    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json(
        { error: 'OpenAI API key not configured. Please add OPENAI_API_KEY to your .env.local file.' }, 
        { status: 500 }
      );
    }

    const model = new ChatOpenAI({
      model: "gpt-4o-mini",
      temperature: 0,
    });

    const outputParser = new HttpResponseOutputParser();

    const chain = prompt.pipe(model).pipe(outputParser);

    const stream = await chain.stream({
      chat_history: formattedPreviousMessages.join("\n"),
      input: currentMessageContent,
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
      },
    });
    
  } catch (e: unknown) {
    console.error('API Error:', e);
    const error = e as Error;
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
}
