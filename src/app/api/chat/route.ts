import { NextRequest, NextResponse } from "next/server";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";

export const runtime = "edge";

// æ ‡å‡†SSEäº‹ä»¶ç±»å‹
enum SSEEventType {
  CONTENT = 'content',
  DONE = 'done', 
  ERROR = 'error',
  METADATA = 'metadata'
}

// æ ‡å‡†SSEæ¶ˆæ¯æ ¼å¼
interface SSEMessage {
  type: SSEEventType;
  data: string;
  id?: string;
  retry?: number;
}

// æ€§èƒ½ç›‘æ§æŒ‡æ ‡
interface PerformanceMetrics {
  requestId: string;
  startTime: number;
  messageCount: number;
  errorCount: number;
}

interface VercelChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

const formatMessage = (message: VercelChatMessage) => {
  return `${message.role}: ${message.content}`;
};

// SSEæ¶ˆæ¯æ„å»ºå‡½æ•°
const buildSSEMessage = (type: SSEEventType, data: string, id?: string): string => {
  const message: SSEMessage = { type, data };
  if (id) message.id = id;
  
  return `data: ${JSON.stringify(message)}\n\n`;
};

// æ€§èƒ½ç›‘æ§å‡½æ•°
const createPerformanceMetrics = (): PerformanceMetrics => ({
  requestId: crypto.randomUUID(),
  startTime: Date.now(),
  messageCount: 0,
  errorCount: 0
});

const logPerformanceMetrics = (metrics: PerformanceMetrics, error?: Error) => {
  const duration = Date.now() - metrics.startTime;
  console.log(JSON.stringify({
    type: 'performance_metrics',
    requestId: metrics.requestId,
    duration,
    messageCount: metrics.messageCount,
    errorCount: metrics.errorCount,
    error: error?.message,
    timestamp: new Date().toISOString()
  }));
};

const TEMPLATE = `ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯å†™ä½œåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€ç»“æ„åŒ–çš„æŠ€æœ¯å›ç­”ï¼Œå°±åƒCursoré‚£æ ·ä¼˜é›…ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡å†…å®¹ï¼Œç»“æ„æ¸…æ™°
2. ä½¿ç”¨ # ## ### ç­‰æ ‡é¢˜å±‚çº§æ¥ç»„ç»‡å†…å®¹
3. ä»£ç å—ä½¿ç”¨ \`\`\`è¯­è¨€ æ ¼å¼ï¼Œç¡®ä¿è¯­æ³•é«˜äº®æ­£ç¡®
4. è¡¨æ ¼ä½¿ç”¨æ ‡å‡†Markdownè¡¨æ ¼è¯­æ³•
5. åˆ—è¡¨ä½¿ç”¨ - æˆ– 1. æ ¼å¼
6. å¼•ç”¨ä½¿ç”¨ > æ ¼å¼
7. å†…è”ä»£ç ä½¿ç”¨ \`code\` æ ¼å¼
8. æä¾›å®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹
9. åŒ…å«ç›¸å…³çš„å¼•ç”¨å’Œå‚è€ƒèµ„æ–™ï¼Œä½¿ç”¨å¤–éƒ¨é“¾æ¥å›¾æ ‡
10. æ ¹æ®å†…å®¹å¤æ‚åº¦è®¾ç½®åˆé€‚çš„éš¾åº¦çº§åˆ«

ç‰¹åˆ«è¦æ±‚ï¼š
- å¯¹äºä»£ç ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨ "## ä»£ç ç¤ºä¾‹" ä½œä¸ºæ ‡é¢˜ï¼Œè¿™æ ·å¯ä»¥è¢«è‡ªåŠ¨è¯†åˆ«ä¸ºå¯æŠ˜å åŒºåŸŸ
- æä¾›å¤šä¸ªä»£ç ç¤ºä¾‹æ—¶ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½è¦æœ‰æ¸…æ™°çš„è¯´æ˜
- é“¾æ¥è¦åŒ…å«æè¿°æ€§æ–‡æœ¬ï¼Œä¸è¦åªæ˜¯URL
- ä½¿ç”¨è¡¨æƒ…ç¬¦å·æ¥å¢å¼ºå¯è¯»æ€§ï¼ˆå¦‚ ğŸ“ ä»£ç ç¤ºä¾‹ï¼ŒğŸ”— å‚è€ƒèµ„æ–™ç­‰ï¼‰

å½“å‰å¯¹è¯:
{chat_history}

ç”¨æˆ·è¾“å…¥: {input}

è¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„å†…å®¹ï¼Œä¸éœ€è¦JSONåŒ…è£…ï¼š`;

export async function POST(req: NextRequest) {
  const metrics = createPerformanceMetrics();
  
  try {
    const body = await req.json();
    const messages: VercelChatMessage[] = body.messages ?? [];
    
    // è¾“å…¥éªŒè¯
    if (!messages.length || !messages[messages.length - 1]?.content) {
      metrics.errorCount++;
      return NextResponse.json(
        { error: 'Invalid request: missing messages or content' }, 
        { status: 400 }
      );
    }

    const formattedPreviousMessages = messages.slice(0, -1).map(formatMessage);
    const currentMessageContent = messages[messages.length - 1].content;
    const prompt = PromptTemplate.fromTemplate(TEMPLATE);

    // æ£€æŸ¥APIå¯†é’¥
    if (!process.env.OPENAI_API_KEY) {
      metrics.errorCount++;
      return NextResponse.json(
        { error: 'OpenAI API key not configured. Please add OPENAI_API_KEY to your .env.local file.' }, 
        { status: 500 }
      );
    }

    const model = new ChatOpenAI({
      model: "gpt-4o-mini",
      temperature: 0.7,
      streaming: true,
      maxTokens: 4000,
      timeout: 30000, // 30ç§’è¶…æ—¶
    });

    const chain = prompt.pipe(model);
    const stream = await chain.stream({
      chat_history: formattedPreviousMessages.join("\n"),
      input: currentMessageContent,
    });

    // åˆ›å»ºæ ‡å‡†SSEæµå¼å“åº”
    const encoder = new TextEncoder();
    
    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          // å‘é€å¼€å§‹å…ƒæ•°æ®
          controller.enqueue(encoder.encode(
            buildSSEMessage(SSEEventType.METADATA, JSON.stringify({
              requestId: metrics.requestId,
              timestamp: new Date().toISOString(),
              model: 'gpt-4o-mini'
            }), metrics.requestId)
          ));

          for await (const chunk of stream) {
            // å¤„ç†LangChainçš„AIMessageChunk
            let content = '';
            if (chunk && typeof chunk === 'object' && 'content' in chunk) {
              content = (chunk as { content: string }).content;
            } else if (typeof chunk === 'string') {
              content = chunk;
            }
            
            if (content && content.trim()) {
              metrics.messageCount++;
              controller.enqueue(encoder.encode(
                buildSSEMessage(SSEEventType.CONTENT, content, `${metrics.requestId}-${metrics.messageCount}`)
              ));
            }
          }
          
          // å‘é€å®Œæˆä¿¡å·
          controller.enqueue(encoder.encode(
            buildSSEMessage(SSEEventType.DONE, '', `${metrics.requestId}-done`)
          ));
          
          controller.close();
          logPerformanceMetrics(metrics);
          
        } catch (error) {
          metrics.errorCount++;
          const errorMessage = error instanceof Error ? error.message : 'Unknown error';
          
          controller.enqueue(encoder.encode(
            buildSSEMessage(SSEEventType.ERROR, errorMessage, `${metrics.requestId}-error`)
          ));
          
          controller.close();
          logPerformanceMetrics(metrics, error as Error);
        }
      },
    });

    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no', // ç¦ç”¨Nginxç¼“å†²
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type',
      },
    });
    
  } catch (e: unknown) {
    metrics.errorCount++;
    const error = e as Error;
    logPerformanceMetrics(metrics, error);
    
    return NextResponse.json({ 
      error: error.message,
      requestId: metrics.requestId 
    }, { status: 500 });
  }
}